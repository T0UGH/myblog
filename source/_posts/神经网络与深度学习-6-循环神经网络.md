---
title: '[神经网络与深度学习][6][循环神经网络]'
date: 2020-11-11 17:15:09
categories:
    - 深度学习
tags:
    - 机器学习
---
## 第6章 循环神经网络



>经验是智慧之父，记忆是智慧之母．——谚语



在**前馈神经网络**中，信息的传递是**单向**的，这种限制虽然使得网络变得更**容易学习**，但在一定程度上也**减弱**了神经网络**模型的能力**．在生物神经网络中，神经元之间的连接关系要复杂得多．**前馈神经网络**可以看作一个**复杂的函数**，每次**输入**都是**独立**的，即网络的**输出** **只依赖** 于当前的**输入**．但是在很多**现实任务**中，网络的输出不仅和当前时刻的输入相关，也**和其过去一段时间的输出相关**．比如一个有限状态自动机，其下一个时刻的状态（输出）不仅仅和当前输入相关，也和当前状态（上一个时刻的输出）相关．此外，**前馈网络难以处理时序数据**，比如视频、语音、文本等．时序数据的**长度**一般是**不固定**的，而**前馈神经网络**要求**输入和输出的维数都是固定**的，不能任意改变．因此，当处理这一类和时序数据相关的问题时，就需要一种能力更强的模型．



循环神经网络（Recurrent Neural Network，RNN）是一类具有**短期记忆能力**的**神经网络**．在循环神经网络中，神经元不但可以**接受其他神经元的信息**，也可以**接受自身的信息**，形成具有**环路**的网络结构．



循环神经网络的**参数学习**可以通过**随时间反向传播算法** 来学习．随时间反向传播算法即**按照时间的逆序**将**错误信息**一步步地**往前传递**．



当**输入序列比较长**时，会存在**梯度爆炸**和**消失问题**，也称为**长程依赖问题**．为了解决这个问题，人们对循环神经网络进行了很多的改进，其中最有效的改进方式引入**门控机制**（Gating Mechanism）．



此外，循环神经网络可以很容易地**扩展到**两种**更广义的记忆网络模型**：**递归神经网络**和**图网络**．



### 6.1 给网络增加记忆能力



为了**处理**这些**时序数据**并**利用其历史信息**， 我们**需要**让**网络具有短期记忆能力**．



一般来讲，我们可以通过以下**三种方法**来给网络**增加短期记忆能力**．



#### 6.1.1 延时神经网络

> 加一个延时存储单元。在**第t 个时刻**，**第𝑙 层神经元**的**活性值**依赖于**第𝑙 − 1 层神经元**的**最近𝐾 个时刻的活性值**，

一种**简单**的**利用历史信息**的方法是**建立一个额外的延时单元**，用来**存储网络的历史信息**（可以包括输入、输出、隐状态等）．比较有代表性的模型是**延时神经网络**



**延时神经网络**是在**前馈网络中**的**非输出层**都**添加**一个**延时器**，**记录**神经元的**最近几次活性值**．在**第t 个时刻**，**第𝑙 层神经元**的**活性值**依赖于**第𝑙 − 1 层神经元**的**最近𝐾 个时刻的活性值**，即
$$
h^{(l)}_t = f(h^{(l−1)}_t , h^{(l−1)}_{t-1} , ⋯ , h^{(l−1)}_{t-k})
$$


#### 6.1.2 有外部输入的非线性自回归模型



**自回归模型**（AutoRegressive Model，AR）是统计学上常用的一类**时间序列模型**，用一个变量$y_t$ 的**历史信息**来**预测自己**．
$$
y_t = w_0 + \sum ^K _{k=1} w_ky_{t-k} + \epsilon_t
$$

- 其中$K$为超参数
- $w_0, ⋯ , w_K$ 为**可学习参数**
- $\epsilon_t ∼ N(0, \sigma^2)$ 为第$t$个时刻的**噪声**，方差$\sigma ^2$和时间无关．



有**外部输入**的**非线性自回归模型**（Nonlinear AutoRegressive with Exogenous Inputs Model，NARX）是**自回归模型**的**扩展**，在**每个时刻t** 都有一个**外部输入$x_t$**，**产生一个输出$y_t$**．**NARX**通过一个**延时器** **记录** 最近$K_x$ 次的**外部输入**和**最近**$K_y$次的**输出**，第t 个时刻的输出𝒚𝑡 为
$$
y_t = 𝑓(x_t, x_{t-1}, ⋯ , x_{t−K_x} , y_{t−1}, y_{t−2}, ⋯ , y_{t−𝐾_y} )
$$
其中**𝑓(⋅)** 表示**非线性函数**，可以是一个前馈网络，$K_x$和$K_y$为**超参数**．



#### 6.1.3 循环神经网络



**循环神经网络**（Recurrent Neural Network，RNN）通过使用**带自反馈的神经元**，能够**处理任意长度的时序数据**．



给定一个输入序列$x_{1∶T} = (x_1, x_2, ⋯ , x_t, ⋯ , x_T )$，循环神经网络通过下面公式**更新** **带反馈边的** **隐藏层的活性值**$h_t$：
$$
h_t = 𝑓(h_{t−1}, x_t)
$$

其中$h_0 = 0$，**𝑓(⋅)** 为一个**非线性函数**，可以是一个前馈网络．



图6.1给出了循环神经网络的示例，其中“**延时器**” 为一个虚拟单元，**记录神经元的最近一次（或几次）活性值**．

![](https://i.loli.net/2020/09/22/QYelyMWuGhkTAXi.png)



循环神经网络可以看成一个**动力系统**．隐藏层的**活性值$h_t$**在很多文献上也称为**状态**（State）或**隐状态**（Hidden State）．



> 动力系统（Dynamical System）是一个数学上的概念，指**系统状态按照一定的规律随时间变化的系统**．具体地讲，动力系统是使用一个**函数**来**描述**一个给定空间（如某个物理系统的状态空间）中**所有点随时间的变化**情况．生活中很多现象（比如钟摆晃动、台球轨迹等）都可以动力系统来描述．



理论上，**循环神经网络**可以**近似**任意的**非线性动力系统**．**前馈神经网络**可以**模拟**任何**连续函数**，而**循环神经网络**可以**模拟**任何**程序**．



### 6.2 简单循环网络



**简单循环网络**一个非常简单的循环神经网络，**只有一个隐藏层的神经网络**．在一个两层的前馈神经网络中，连接存在于相邻的层与层之间，同一隐藏层的节点之间是无连接的．而简单循环网络增加了隐藏层之间的反馈连接．



令向量$x_t \in R^M$ 表示在**时刻$t$**时网络的**输入**，$h_t \in R^D$ 表示**隐藏层状态**（即隐藏层神经元活性值），则$h_t$不仅**和当前时刻的输入**$x_t$ 相关，也和**上一个时刻的隐藏层状态**$𝒉_{t−1}$ 相关．**简单循环网络**在时刻t的**更新公式**为
$$
\boldsymbol z_t = U \boldsymbol h_{t−1} + W \boldsymbol x_t + \boldsymbol b,
$$

$$
h_t = f(z_t)
$$

- 其中$z_t$为隐藏层的**净输入**
- $U \in R^{𝐷×𝐷}$为**状态-状态权重矩阵**
- $W \in R^{D×M}$ 为**状态-输入权重矩阵**
- $b \in R^D$ 为**偏置向量**
- $f(⋅)$ 是**非线性激活函数**，通常为Logistic函数或Tanh 函数



上述公式通常表示为
$$
\boldsymbol h_t = f(U\boldsymbol h_{t-1} + W\boldsymbol x_t + \boldsymbol b)
$$




如果我们把每个时刻的状态都看作前馈神经网络的一层，**循环神经网络**可以看作在**时间维度**上**权值共享**的**神经网络**．图6.2给出了**按时间展开**的循环神经网络．

![](https://i.loli.net/2020/09/22/F1rdZSWBl6sqyUI.png)



#### 6.2.1 循环神经网络的计算能力



##### 6.2.1.1 循环神经网络的通用近似定理



循环神经网络的拟合能力也十分强大．一个完全连接的循环网络是任何非线性动力系统的近似器．

![](https://i.loli.net/2020/09/22/CU7dEGkenaLpMxN.png)



##### 6.2.1.2 图灵完备



>**图灵机**是一种抽象的信息处理装置，可以用来**解决所有的可计算问题**



**图灵完备**（Turing Completeness）是指一种**数据操作规则**，比如一种计算机编程语言，可以实现**图灵机**（Turing Machine）的**所有功能**，**解决所有的可计算问题**．



![](https://i.loli.net/2020/09/22/HdmROTaKCNky8ps.png)



**因此，一个完全连接的循环神经网络可以近似解决所有的可计算问题．**



### 6.3 应用到机器学习



**循环神经网络**可以应用到**很多不同类型**的**机器学习任务**．



根据这些任务的特点可以分为以下几种模式：

- **序列到类别模式**
- **同步的序列到序列模式**
- **异步的序列到序列模式**



#### 6.3.1 序列到类别模式



**序列到类别模式**主要用于**序列数据**的**分类问题**：输入为序列，输出为类别．比如在**文本分类**中，输入数据为单词的序列，输出为该文本的类别．



假设一个**样本** $x_{1∶T} = (x_1, ⋯ , x_T)$ 为一个**长度为$T$**的**序列**，**输出**为一个**类别** $y \in (1, ⋯ , C)$．我们可以将样本$x$ **按不同时刻输入**到循环神经网络中，并**得到不同时刻**的隐藏状态$h_1, ⋯ , h_T$．我们可以**将$h_T$ 看作整个序列的最终表示**（或特征），并**输入给分类器**$g(.)$ 进行**分类**（如图6.3a所示），即

![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201016145823.png)



除了将最后时刻的状态作为整个序列的表示之外，我们还可以对整个序列的**所有状态**进行**平均**，并用这个**平均状态**来作为整个序列的**表示**（如图6.3b所示），即
$$
\hat y = g(\frac 1 T \sum ^T _{t=1} \boldsymbol h_t)
$$
![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201016150112.png)

#### 6.3.2 同步的序列到序列模式



**同步的序列到序列模式**主要用于**序列标注**（Sequence Labeling）任务，即每一时刻都有输入和输出，**输入序列**和**输出序列**的**长度相同**．比如在**词性标注**（Part-of-Speech Tagging）中，每一个单词都需要标注其对应的词性标签．



在**同步的序列到序列模式**（如图6.4所示）中，**输入**为一个**长度为$T$**的序列$x_{1∶T} = (x_1, ⋯ , x_T)$，**输出**为**序列**$y_{1∶T} = (y_1, ⋯ , y_T)$．**样本𝒙** 按**不同时刻输入**到**循环神经网络**中，并**得到不同时刻的隐状态**$h_1, ⋯ , h_T$．每个时刻的隐状态$h_t$代表了当前时刻和历史的信息，并**输入给分类器**$g(.)$得到当前时刻的标签$\hat y_t$，即

![](https://i.loli.net/2020/09/23/FE3begK41qjIMhZ.png)

#### 6.3.3 异步的序列到序列模式



**异步的序列到序列模式**也称为**编码器-解码器（Encoder-Decoder）模型**，即输入序列和输出序列**不需要有严格的对应关系**，也**不需要保持相同的长度**．比如在机器翻译中，输入为源语言的单词序列，输出为目标语言的单词序列．



在异步的序列到序列模式中，输入为长度为$T$的序列$x_{1∶T} = (x_1, ⋯ , x_T )$，输出为长度为$M$的序列$y_{1:M} = (y_1, ..., y_M)$．



异步的序列到序列模式一般通过**先编码** **后解码**的方式来实现．

1. 先**将样本𝒙**  **按不同时刻** **输入**到一个循环神经网络（**编码器**）中，并**得到其编码**$h_𝑇$．
2. 然后再使用**另一个循环神经网络**（解码器），得到**输出序列**$y_{1∶M}$．为了建立输出序列之间的依赖关系，在**解码器**中通常使用**非线性的自回归模型**．



令$f_1(⋅)$和$f_2(⋅)$ 分别为用作**编码器**和**解码器**的**循环神经网络**，则编码器-解码器模型可以写为

![](https://i.loli.net/2020/09/23/PUGfQtCHKk1NlVz.png)

![](https://i.loli.net/2020/09/23/AqsNujQ39xSrJKX.png)



### 6.4 参数学习



**循环神经网络的参数**可以通过**梯度下降**方法来进行学习．



以随机梯度下降为例，给定一个训练样本$(\boldsymbol x, \boldsymbol y)$，其中$\boldsymbol x_{1:T} = (\boldsymbol x_1, ... ,\boldsymbol x_T)$为长度是𝑇 的输入序列，𝑦1∶𝑇 = (𝑦1, ⋯ , 𝑦𝑇 ) 是长度为𝑇 的标签序列．即在每个时刻$t$，都有一个监督信息$y_t$，我们定义时刻$t$的损失函数为
$$
L_t = L(y_t, g(\boldsymbol h_t))
$$

- 其中$g(\boldsymbol h_t)$为第$t$时刻的输出，
- $L$为可微分的损失函数，比如交叉熵．



那么整个序列的损失函数为
$$
L = \sum ^T _{t=1} L_t
$$


整个序列的损失函数$L$关于参数$U$的梯度为
$$
\frac {\delta L} {\delta \boldsymbol U} = \sum ^T _{t=1} \frac {\delta L_t} {\delta \boldsymbol U}
$$
即**每个时刻损失**$L_t$**对参数**$U$的**偏导数之和**



在循环神经网络中主要有两种计算梯度的方式：

- **随时间反向传播（BPTT）算法**
- **实时循环学习（RTRL）算法**



#### 6.4.1 随时间反向传播算法



随时间反向传播（BackPropagation Through Time，BPTT）算法的主要思想是通过类似前馈神经网络的错误反向传播算法来计算梯度．



BPTT 算法将循环神经网络看作一个**展开的多层前馈网络**，

- 其中“每一层”对应循环网络中的“每个时刻”（见图6.2）．
- 这样，循环神经网络就可以按照前馈网络中的反向传播算法计算参数梯度．
- 在“展开”的前馈网络中，所有层的**参数**是**共享**的，因此参数的**真实梯度**是**所有“展开层”的参数梯度之和**．



![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201022105021.png)

##### 6.4.1.1 计算偏导数$\frac {\delta L_t} {\delta U}$



因为参数$U$和隐藏层在每个时刻$k(1 \leq k \leq t)$ 的**净输入**$\boldsymbol z_k = U\boldsymbol h_{k-1} + W\boldsymbol x_k + \boldsymbol b$有关，因此第$t$时刻的损失函数$L_t$关于参数$u_{i,j}$的梯度为：
$$
\frac {\delta L_t} {\delta u_{i,j}} = \sum ^t _{k=1} \frac {\delta  ^+ \boldsymbol z_t} {\delta u_{i,j}} \frac {\delta L_t} {\delta \boldsymbol z_t} （6.31）
$$
![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201022110934.png)

定义误差项$\delta_{t,k} = \frac {\delta L_t} {\delta {\boldsymbol z_k}}$为**第$t$时刻的损失**对**第$k$时刻隐藏神经层的净输入$\boldsymbol z_k$的导数**，则当$1 \leq k \leq t$时

![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201022110609.png)



下图给出了**误差项** **随时间进行反向传播算法**的示例．

![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201022111049.png)

##### 6.4.1.2 参数梯度

整个序列的损失函数$L$关于参数$U$的梯度
$$
\frac {\delta L} {\delta {\boldsymbol U}} = \sum ^T _{t=1} \sum ^t _{k=1} \delta_{t,k} \boldsymbol h^T_{k-1}
$$
同理可得，$L$关于权重$\boldsymbol W$和偏置$\boldsymbol b$的梯度为
$$
\frac {\delta L} {\delta {\boldsymbol W}} = \sum ^T _{t=1} \sum ^t _{k=1} \delta_{t,k} \boldsymbol x^T_{k}
$$

$$
\frac {\delta L} {\delta {\boldsymbol b}} = \sum ^T _{t=1} \sum ^t _{k=1} \delta_{t,k}
$$



##### 6.4.1.3 计算复杂度



在**BPTT 算法**中，**参数的梯度**需要在一个完整的**“前向”计算**和**“反向”计算**后才能得到并进行参数更新．



#### 6.4.2 实时循环学习算法



与反向传播的BPTT 算法不同的是，**实时循环学习**（Real-Time Recurrent Learning，**RTRL**）是**通过前向传播**的方式来**计算梯度**

假设循环神经网络中第$t+1$时刻的状态$\boldsymbol h_{t+1}$为
$$
\boldsymbol h_{t+1} = f(\boldsymbol z_{t+1}) = f(\boldsymbol U \boldsymbol h_t + \boldsymbol W \boldsymbol x_{t+1} + \boldsymbol b)
$$
其关于参数$u_{i,j}$的偏导数为

![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201022114542.png)



RTRL 算法从第1个时刻开始，**除了计算**循环神经网络的**隐状态**之外，还利用公式(6.45) 依次前向计算偏导数$\frac {\delta \boldsymbol h_1} {\delta u_{i,j}}$、$\frac {\delta \boldsymbol h_2} {\delta u_{i,j}}$

、$\frac {\delta \boldsymbol h_3} {\delta u_{i,j}}$，...



这样，假设第$t$个时刻存在一个监督信息，其损失函数为$L_t$，就可以同时计算损失函数对$u_{i,j}$的偏导数
$$
\frac {L_t} {\delta u_{i,j}} = \frac {\delta \boldsymbol h_t} {\delta u_{i,j}} \frac {\delta L_t} {\delta \boldsymbol h_t}
$$
这样在第$t$时刻，可以**实时地计算**损失$L_t$关于参数$U$的**梯度**，**并更新参数**．参数$\boldsymbol W$和$\boldsymbol b$的梯度也可以同样按上述方法实时计算．



#### 6.4.3 两种算法的比较



**RTRL**算法和**BPTT**算法都是**基于梯度下降的算法**，分别**通过前向模式**和**反向模式**应用**链式法则**来**计算梯度**．

- 在**循环神经网络**中，一般网络输出维度远低于输入维度，因此**BPTT** 算法的**计算量会更小**，但是BPTT 算法需要保
  存所有时刻的中间梯度，**空间复杂度较高**．
- **RTRL** 算法**不需要梯度回传**，因此非常**适合**用于需要**在线学习**或**无限序列**的任务中．

 

### 6.5 长程依赖问题



循环神经网络在学习过程中的主要问题是由于**梯度消失**或**爆炸**问题，**很难建模长时间间隔**（Long Range）的状态之间的依赖关系．



在BPTT 算法中，将公式(6.36)

![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201022120640.png)

 展开得到

![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201022120726.png)



如果定义$ \gamma \cong ||diag(f'(\boldsymbol z _{\tau} )U^T||$，则
$$
\delta _{t,k} \cong \gamma^{t-k} \delta_{t,t}
$$


- 若𝛾 > 1，当𝑡 − 𝑘 → ∞ 时，$\gamma^{t−k} → ∞$．当间隔𝑡 − 𝑘 比较大时，梯度也变得很大，会造成系统不稳定，称为**梯度爆炸问题**（Gradient Exploding Problem）．
- 相反，若𝛾 < 1，当𝑡 − 𝑘 → ∞ 时，$\gamma^{t-k} → 0$．当间隔𝑡 − 𝑘 比较大时，梯度也变得非常小，会出现和深层前馈神经网络类似的**梯度消失问题**（Vanishing Gradient Problem)



>要注意的是，在循环神经网络中的梯度消失不是说$\frac {\delta L_t} {\delta U}$的梯度消失了，而是$\frac {\delta L_t} {\delta h_k}$的梯度消失了（当间隔$t-k$比较大时）．也就是说，**参数𝑼 的更新**主要靠**当前时刻$t$的几个相邻状态$h_k$来更新**，**长距离的状态**对参数$U$**没有影响**．



由于**循环神经网络**经常使用非线性激活函数为Logistic 函数或Tanh 函数作为非线性激活函数，其**导数值都小于1**，并且权重矩阵$‖U‖$也不会太大，因此如果**时间间隔**$t-k$过大，$\delta _{t,k}$会**趋向于0**，因而**经常会出现梯度消失问题**．



虽然简单循环网络**理论上可以建立长时间间隔的状态之间的依赖关系**，**但是**由于梯度爆炸或消失问题，**实际上只能学习到短期的依赖关系．**



这样，如果时刻$t$的输出$\boldsymbol y_t$依赖于时刻$k$的输入$\boldsymbol x_k$，当间隔$t-k$比较大时，简单神经网络很难建模这种长距离的依赖关系，称为**长程依赖问题**（Long-Term Dependencies Problem）．



#### 6.5.1 改进方案



比较有效的方式是通过改进模型或优化方法来缓解循环网络的梯度爆炸和梯度消失问题．



##### 6.5.1.1 梯度爆炸的改进

一般而言，循环网络的**梯度爆炸**问题比较容易解决，一般通过**权重衰减**或**梯度截断**来**避免**．

**权重衰减**是通过给参数增加$l_1$或$l_2$范数的**正则化**项来限制参数的取值范围，从而使得𝛾 ≤ 1．

**梯度截断**是另一种有效的启发式方法，当梯度的模大于一定阈值时，就将它截断成为一个较小的数．



##### 6.5.1.2 梯度消失的改进



梯度消失是循环网络的主要问题．除了使用一些优化技巧外，更有效的方式就是**改变模型**



可以将$\boldsymbol h_t = f(U \boldsymbol h_{t-1} +W \boldsymbol x_t + \boldsymbol b)$中的$U = I$，同时令$\frac {\delta h_t} {\delta h_{t-1}} = I$为单位矩阵，即
$$
\boldsymbol h_t = \boldsymbol h_{t-1} + g(\boldsymbol x_t;\theta)
$$

- $h_t$和$h_{t-1}$之间为**线性依赖关系**，且**权重系数为1**，这样就**不存在梯度爆炸或消失问题**．
- 但是，这种改变也**丢失了**神经元在反馈边上的**非线性**激活的**性质**，因此也降低了模型的表示能力．



为了避免这个缺点，我们可以采用一种更加有效的改进策略：
$$
\boldsymbol h_t = \boldsymbol h_{t-1} + g(\boldsymbol x_t;\boldsymbol h_{t-1}; \theta)
$$
这样$h_t$和$h_{t-1}$之间为**既有线性关系**，**也有非线性关系**，并且可以缓解梯度消失问题．



但这种改进依然存在两个问题：

1. **梯度爆炸问题**
2. **记忆容量问题**：随着$h_t$不断累积存储新的输入信息，会发生**饱和现象**．假设$g(.)$为Logistic 函数，则随着时间$t$的增长，$\boldsymbol h_t$会变得越来越大，从而导致$h$变得饱和．也就是说，隐状态$h_t$可以**存储的信息是有限的**，随着记忆单元存储的内容越来越多，其丢失的信息也越来越多．

**logistic函数**

![](https://i.loli.net/2020/10/23/KLYm1wojPp4TreS.png)



### 6.6 基于门控的循环神经网络



为了**改善**循环神经网络的**长程依赖问题**， 一种非常好的解决方案是在公式(6.50)的基础上**引入门控机制**来**控制**信息的**累积速度**，包括**有选择地加入**新的信息，并**有选择地遗忘**之前累积的信息



主要介绍两种基于门控的循环神经网络：

- **长短期记忆网络**(LSTM)
- **门控循环单元网络**(GRU)



#### 6.6.1 长短期记忆网络



在公式$\boldsymbol h_t = \boldsymbol h_{t-1} + g(\boldsymbol x_t;\boldsymbol h_{t-1}; \theta)$的基础上，LSTM 网络主要改进在以下两个方面



##### 6.6.1.1 新的内部状态



LSTM 网络引入一个新的**内部状态**（internal state）$\boldsymbol c_t \in R^D$．内部状态$\boldsymbol c_t$通过下面公式计算：
$$
\boldsymbol c_t = \boldsymbol f_t \odot \boldsymbol c_{t-1} + \boldsymbol i_t \odot \boldsymbol {\hat c_t}
$$

$$
\boldsymbol h_t = \boldsymbol o_t \odot tanh(\boldsymbol c_t)
$$

- 其中$\boldsymbol f_t \in [0,1]^D$、$\boldsymbol i_t \in [0,1]^D$、$\boldsymbol o_t \in [0,1]^D$为三个**门**(gate)来控制信息传递的路径。
- $\odot$为**向量元素乘积**；
- $\boldsymbol c_{t-1}$为**上一时刻的记忆单元**；
- $\boldsymbol {\hat c_t} \in R^D$是**通过非线性函数**得到的**候选状态**：$\boldsymbol {\hat c_t} = tanh(U_c\boldsymbol h_{t-1} + W_c\boldsymbol x_t + \boldsymbol b_c)$



在每个时刻$t$，LSTM网络的内部状态$\boldsymbol c_t$记录了到当前时刻为止的历史信息．



##### 6.6.1.2 门控机制



> 在数字电路中，门（gate）为一个二值变量$\{0, 1\}$，0 代表关闭状态，不许任何信息通过；1 代表开放状态，允许所有信息通过．



LSTM 网络引入**门控机制**（Gating Mechanism）来控制信息传递的路径．公式(6.51) 和公式(6.52) 中三个“门”分别为**输入门**$\boldsymbol i_t$、**遗忘门**$\boldsymbol f_t$和**输出门**$\boldsymbol o_t$．这三个门的作用为

1. **遗忘门**$f_t$控制**上一个时刻的内部状态**$c_{t-1}$需要**遗忘多少信息**．
2. **输入门**$i_t$控制当前时刻的**候选状态**$c_t$有多少信息需要**保存**．
3. **输出门**$o_t$ 控制当前时刻的**内部状态**$c_t$有多少信息需要**输出给外部状态**$h_t$．



例如：

- 当$f_t = 0, i_t = 1$时，记忆单元将**历史信息清空**，并将候选状态向量$\boldsymbol {\hat c_t}$写入．
- 当$f_t = 1, i_t = 0$时，记忆单元将**复制上一时刻的内容**，**不写入新的信息**．



LSTM 网络中的“门”是一种**“软”门**，取值**在(0, 1) 之间**，表示**以一定的比例允许信息通过**．



三个门的计算方式为：
$$
\boldsymbol {i_t} = \sigma(U_i\boldsymbol h_{t-1} + W_i\boldsymbol x_t + \boldsymbol b_i)
$$

$$
\boldsymbol {f_t} = \sigma(U_f\boldsymbol h_{t-1} + W_f\boldsymbol x_t + \boldsymbol b_f)
$$

$$
\boldsymbol {o_t} = \sigma(U_o\boldsymbol h_{t-1} + W_o\boldsymbol x_t + \boldsymbol b_o)
$$

- 其中𝜎(⋅) 为Logistic 函数，其输出区间为(0, 1)，
- 𝒙𝑡 为当前时刻的输入，
- 𝒉𝑡−1 为上一时刻的外部状态．



下图给出了 LSTM 网络的循环单元结构，其计算过程为：

1. 首先利用上一时刻的外部状态$\boldsymbol h_{t-1}$和当前时刻的输入$\boldsymbol x_t$，计算出三个门，以及候选状态$\boldsymbol {\hat c_t}$；
2. 结合遗忘门$\boldsymbol f_t$和输入门$\boldsymbol i_t$来更新记忆单元$\boldsymbol c_t$；
3. 结合输出门$\boldsymbol i_t$，将内部状态的信息传递给外部状态$\boldsymbol h_t$．



![](https://i.loli.net/2020/10/23/6hwR5nHmJ2xZiTj.png)



#### 6.6.2 LSTM网络的各种变种



##### 6.6.2.1 无遗忘门的LSTM 网络



**最早**提出的LSTM 网络是**没有遗忘门**的，其内部状态的更新为
$$
\boldsymbol c_t = \boldsymbol c_{t-1} + \boldsymbol i_t \odot \boldsymbol {\hat c_t}
$$


记忆单元𝒄 会不断增大．当输入序列的**长度非常大**时，**记忆单元的容量会饱和**，从而大大降低LSTM 模型的性能．



##### 6.6.2.2 peephole 连接



另外一种变体是**三个门**不但**依赖于输入**$\boldsymbol x_t$ 和上一时刻的**隐状态**$\boldsymbol h_{t-1}$，也依赖于**上一个时刻的记忆单元**$\boldsymbol c_{t-1}$，即
$$
\boldsymbol {i_t} = \sigma(U_i\boldsymbol h_{t-1} + W_i\boldsymbol x_t + V_i\boldsymbol c_{t-1} + \boldsymbol b_i)
$$

$$
\boldsymbol {f_t} = \sigma(U_f\boldsymbol h_{t-1} + W_f\boldsymbol x_t + V_f\boldsymbol c_{t-1} + \boldsymbol b_f)
$$

$$
\boldsymbol {o_t} = \sigma(U_o\boldsymbol h_{t-1} + W_o\boldsymbol x_t + V_o\boldsymbol c_{t-1} + \boldsymbol b_o)
$$

##### 6.6.2.2 将输入门和遗忘门耦合

LSTM 网络中的输入门和遗忘门有些互补关系，因此同时用两个门比较冗余．为了**减少LSTM 网络的计算复杂度**，将这两门**合并为一个门**．令$\boldsymbol f_t = \boldsymbol 1 - \boldsymbol i_t$，内部状态的更新方式为
$$
\boldsymbol c_t = (\boldsymbol 1 - \boldsymbol i_t) \odot \boldsymbol c_{t-1} + \boldsymbol i_t \odot \boldsymbol {\hat c_t}
$$


#### 6.6.3 门控循环单元网络（Gated Recurrent Unit，GRU）



GRU 网络引入**门控机制**来控制信息更新的方式．和LSTM 不同，GRU 不引入额外的记忆单元，GRU 网络也是在公式$\boldsymbol h_t = \boldsymbol h_{t-1} + g(\boldsymbol x_t, \boldsymbol h_{t-1}; \theta)$的基础上引入一个**更新门**（Update Gate）来**控制当前状态**需要**从历史状态中保留多少信息**，以及需要**从候选状态中接受多少新信息**，即
$$
\boldsymbol h_t = \boldsymbol z_t \odot \boldsymbol h_{t-1} +(1 - \boldsymbol z_t) \odot g(\boldsymbol x_t, \boldsymbol h_{t-1}; \theta)
$$
其中$\boldsymbol z_t \in [0,1]^D$为**更新门**：
$$
\boldsymbol z_t = \sigma(W_z \boldsymbol x_t + U_z \boldsymbol h_{t-1} + \boldsymbol b_z)
$$
在**LSTM 网络**中，输入门和遗忘门是互补关系，具有一定的**冗余性**．**GRU 网络**直接使用**一个门**来**控制输入和遗忘之间的平衡**．

- 当$\boldsymbol z_t = \boldsymbol 0$时，当前状态$\boldsymbol h_t$和前一时刻的状态$\boldsymbol h_{t-1}$之间为**非线性函数**关系；
- 当$\boldsymbol z_t = \boldsymbol 0$时，$\boldsymbol h_t$和$\boldsymbol h_{t-1}$之间为**线性函数**关系．



在GRU 网络中，函数$g(\boldsymbol x_t, \boldsymbol h_{t-1}; \theta) $的定义为
$$
\boldsymbol {\hat h_t} = g(\boldsymbol x_t, \boldsymbol h_{t-1}; \theta)   =  tanh(\boldsymbol W_h\boldsymbol x_t + \boldsymbol U_h(\boldsymbol r_t \odot \boldsymbol h_{t-1}) + \boldsymbol b_n)
$$

- 其中$\boldsymbol {\hat h_t}$表示**当前时刻的候选状态**
- $\boldsymbol r_t \in [0, 1]^D$为**重置门**：$\boldsymbol r_t =\sigma(W_r \boldsymbol x_t + U_r \boldsymbol h_{t-1} + \boldsymbol b_r) $



$\boldsymbol r_t$用来控制候选状态$\boldsymbol {\hat h_t}$的计算是否依赖上一时刻的状态$\boldsymbol {\hat h_{t-1}}$

- **当$\boldsymbol r_t = 0$时**，候选状态$\boldsymbol {\hat h_t} = tanh(\boldsymbol W_h\boldsymbol x_t + \boldsymbol b_n)$，即**只和当前输入$\boldsymbol x_t$相关**，**和历史状态无关．**
- **当$\boldsymbol r_t = 1$时**，候选状态$\boldsymbol {\hat h_t} = tanh(\boldsymbol W_h\boldsymbol x_t + \boldsymbol U_h \boldsymbol h_{t-1}  + \boldsymbol b_n)$和当前输入𝒙𝑡 以及历史状态𝒉𝑡−1 相关，**和简单循环网络一致．**



下图给出了GRU 网络的循环单元结构．

![image-20201023204148468](C:\Users\MSI-NB\AppData\Roaming\Typora\typora-user-images\image-20201023204148468.png)



### 6.7 深层循环神经网络



如果将**深度**定义为**网络中信息传递路径长度**的话，**循环神经网络**可以看作**既“深”又“浅”**的网络．

- 一方面来说，如果我们把循环网络**按时间展开**，长时间间隔的状态之间的**路径很长**，循环网络可以看作一个非常深的网络．
- 从另一方面来说，如果**同一时刻**网络**输入到输出之间的路径**$\boldsymbol x_t \rarr \boldsymbol y_t$，这个网络是非常**浅**的．



因此，我们可以**增加循环神经网络的深度**从而增强循环神经网络的能力．增加循环神经网络的深度主要是**增加同一时刻网络输入到输出之间的路径**$\boldsymbol x_t \rarr \boldsymbol y_t$，

- 比如增加隐状态到输出$\boldsymbol h_t \rarr \boldsymbol y_t$
- 以及输入到隐状态$\boldsymbol x_t \rarr \boldsymbol h_t$之间的路径的深度



#### 6.7.1 堆叠循环神经网络



一种常见的**增加循环神经网络深度的做法**是**将多个循环网络堆叠起来**，称为**堆叠循环神经网络**（Stacked Recurrent Neural Network，SRNN）．



下图给出了按时间展开的堆叠循环神经网络．第$l$层网络的输入是第$l-1$层网络的输出．我们定义$\boldsymbol h^{(l)}_t$为**在时刻$t$时第$l$层的隐状态**
$$
\boldsymbol h_t ^{(l)} = f(U^{(l)}\boldsymbol h^{(l)}_{t-1} + W^{(l)}\boldsymbol h^{(l-1)}_t + \boldsymbol b^{(l)})
$$
其中$U^{(l)}$、$W^{(l)}$和$\boldsymbol b^{(l)}$为权重矩阵和偏置向量，$\boldsymbol h^{(0)}_t = \boldsymbol x_t$

![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201023205153.png)



#### 6.7.2 双向循环神经网络



在有些任务中，一个时刻的**输出**不但**和过去时刻的信息有关**，也**和后续时刻的信息有关**．



> 比如给定一个句子，其中一个词的词性由它的上下文决定，即包含左右两边的信息．



因此，在这些任务中，我们可以**增加一个按照时间的逆序来传递信息的网络层**，来增强网络的能力．



双向循环神经网络（Bidirectional Recurrent Neural Network，Bi-RNN）由**两层循环神经网络组成**，它们的**输入相同**，只是**信息传递的方向不同**．



假设第1 层按时间顺序，第2 层按时间逆序，在时刻$t$时的隐状态定义为$\boldsymbol h_t^{(1)}$和$\boldsymbol h^{(2)}_t$ ，则
$$
\boldsymbol h_t^{(1)} = f(U^{(1)} \boldsymbol h^{(1)}_{t-1} + W^{(1)} \boldsymbol x_t + \boldsymbol b^{(1)})
$$

$$
\boldsymbol h_t^{(2)} = f(U^{(2)}\boldsymbol h^{(2)}_{t+1} + W^{(2)}\boldsymbol x_t + \boldsymbol b^{(2)})
$$

$$
\boldsymbol h_t = \boldsymbol h_t^{(1)} \oplus \boldsymbol h_t^{(2)} 
$$

其中$\oplus$为**向量拼接操作**．



下图给出了按时间展开的双向循环神经网络．

![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201023210123.png)

### 6.8 扩展到图结构



如果将**循环神经网络** **按时间展开**，每个时刻的隐状态$h_t$看作一个节点，那么这些节点**构成一个链式结构**，每个节点$t$都收到其父节点的消息（Message），更新自己的状态，并传递给其子节点．而**链式结构是一种特殊的图结构**，我们可以比较容易地将这种**消息传递**（Message Passing）的思想**扩展到任意的图结构上**．



#### 6.8.1 递归神经网络



**递归神经网络**（Recursive Neural Network，**RecNN**）是循环神经网络在**有向无循环图**上的扩展



递归神经网络的一般结构为**树状的层次结构**，如图6.11a所示．

![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201023210452.png)





**递归神经网络**主要用来**建模自然语言句子的语义**[Socher et al., 2011, 2013]．给定一个句子的语法结构（一般为树状结构），可以使用递归神经网络来按照句法的组合关系来合成一个句子的语义．句子中每个短语成分又可以分成一些子成分，即每个短语的语义都可以由它的子成分语义组合而来，并进而合成整句的语义．



对于一个节点$\boldsymbol h_i$，它可以接受来自父节点集合$\pi_i$中所有节点的消息，并更新自己的状态．
$$
\boldsymbol h_i = f(\boldsymbol h_{\pi_i})
$$

- 其中$\boldsymbol h_{\pi_i}$表示集合$\pi_i$中**所有节点状态**的**拼接**
- $f(.)$是一个和节点位置无关的**非线性函数**，可以为一个单层的前馈神经网络



比如上图所示的递归神经网络具体可以写为



![](https://healthlung.oss-cn-beijing.aliyuncs.com/20201023211054.png)



- 其中$\sigma (.)$表示**非线性激活函数**
- $W$和$\boldsymbol b$是**可学习的参数**．





同样，输出层𝑦 可以为一个分类器
$$
y = g(W'\boldsymbol h_3 + \boldsymbol b')
$$


同样，我们也可以用**门控机制**来**改进递归神经网络**中的长距离依赖问题，比如树结构的长短期记忆模型就是将LSTM 模型的思想应用到树结构的网络中，来实现更灵活的组合函数．



#### 6.8.2 图神经网络



在实际应用中，很多**数据**是**图结构**的，比如知识图谱、社交网络、分子网络等．而**前馈网络**和**反馈网络**很**难处理图结构的数据**．



**图神经网络**（Graph Neural Network，GNN）是将**消息传递**的思想**扩展到图结构数据上的神经网络**．



对于一个任意的图结构$G(v, \varepsilon)$，

- 其中𝒱 表示节点集合，ℰ 表示边集合．
- 每条边表示两个节点之间的依赖关系．
- 节点之间的连接可以是有向的，也可以是无向的．
- 图中每个**节点𝑣** 都用一组神经元来表示其**状态𝒉(𝑣)**，初始状态可以为节点𝑣的输入特征𝒙(𝑣)．
- 每个节点可以**收到来自相邻节点的消息**，并更新自己的状态．


$$
\boldsymbol m^{(v)}_t = \sum _{u \in N(v)} f(\boldsymbol h^{(v)}_{t-1}, h^{(u)}_{t-1}, \boldsymbol e^{(e,v)})
$$

$$
\boldsymbol h^{(v)}_t = g(\boldsymbol h^{(v)}_{t-1}, \boldsymbol m^{(v)}_t)
$$

- 𝒩(𝑣) 表示节点𝑣 的邻居集合
- $\boldsymbol m^{(v)}_t$表示在第$t$时刻节点$v$收到的信息
- $\boldsymbol e^{(u,v)}$为边$e^{(u,v)}$上的特征．



在整个图更新$T$次后，可以通过一个**读出函数**（Readout Function）$g(.)$来得到整个网络的表示：
$$
\boldsymbol o_t = g(\{\boldsymbol h^{(v)} _T | v \in V\})
$$