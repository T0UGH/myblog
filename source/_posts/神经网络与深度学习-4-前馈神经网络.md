---
title: '[神经网络与深度学习][4][前馈神经网络]'
date: 2020-11-11 17:14:00
categories:
    - 深度学习
tags:
    - 机器学习
---
## 第4章 前馈神经网络



>**神经网络**是一种大规模的**并行分布式处理器**，天然具有存储并使用经验知识的能力．它从两个方面上模拟大脑：（1）**网络**
>**获取的知识**是通过**学习**来获取的；（2）内部**神经元的连接强度**，即突触权重，用于**储存获取的知识**．





**人工神经网络**（Artificial Neural Network，ANN）是指一系列受生物学和神经科学启发的数学模型．这些模型主要是通过**对人脑的神经元网络进行抽象**，**构建人工神经元**，并按照一定拓扑结构来建立人工神经元之间的连接，来模拟生物神经网络．



神经网络最早是作为一种主要的**连接主义模型**．20 世纪80 年代中后期，最流行的一种连接主义模型是**分布式并行处理**

1. **信息表示**是**分布式**的（非局部的）；
2. **记忆和知识**是**存储在单元之间的连接上**；
3. 通过**逐渐改变单元之间的连接强度**来学习新的知识．



从机器学习的角度来看，神经网络一般可以看作一个**非线性模型**，其**基本组成单元**为**具有非线性激活函数**的**神经元**，通过大量神经元之间的连接，使得神经网络成为一种**高度非线性**的模型．**神经元之间的连接权重**就是需要学习的参数，可以在机器学习的框架下通过**梯度下降方法**来进行**学习**．



### 4.1 神经元



神经元（Neuron），是构成神经网络的基本单元。



一个生物神经元通常具有多个树突和一条轴突．

- 树突用来接收信息，轴突用来发送信息．
- 当神经元所获得的输入信号的积累超过某个阈值时，它就处于兴奋状态，产生电脉冲．
- 轴突尾端有许多末梢可以给其他神经元的树突产生连接（突触），并将电脉冲信号传递给其他神经元．



假设一个神经元接收𝐷 个输入𝑥1, 𝑥2, ⋯ , 𝑥𝐷，令向量𝒙 = [𝑥1; 𝑥2; ⋯ ; 𝑥𝐷] 来表示这组输入，并用**净输入**（Net Input）𝑧 ∈ ℝ 表示一个**神经元**所获得的**输入信号**𝒙 的**加权和**
$$
z = \sum ^D _{d=1} w_dx_d + b = 𝒘^T𝒙 + b
$$


其中$w = [𝑤^1; 𝑤^2; ⋯ ; 𝑤^𝐷] ∈ ℝ^𝐷$ 是𝐷 维的**权重向量**，𝑏 ∈ ℝ 是**偏置**．



**净输入**z 在**经过**一个**非线性函数**𝑓(⋅) 后，得到神经元的**活性值**（Activation）a，
$$
a = f(z)
$$

其中**非线性函数𝑓(⋅)** 称为**激活函数**（Activation Function）．

![](https://i.loli.net/2020/09/19/Aa3Zmy2NDXeTGxt.png)

**激活函数**需要具有以下**特征**

1. **连续**并**可导**（允许少数点上不可导）的**非线性函数**．可导的激活函数可以直接利用数值优化的方法来学习网络参数．
2. **激活函数**及其**导函数**要尽可能的**简单**，有利于提高网络计算效率．
3. 激活函数的导函数的**值域**要在一个**合适的区间**内，不能太大也不能太小，否则会影响训练的效率和稳定性．



#### 4.1.1 Sigmoid型函数



Sigmoid 型函数是指一类**S 型曲线函数**，为**两端饱和**函数．常用的Sigmoid型函数有Logistic 函数和Tanh 函数．



##### 4.1.1.1 Logistic函数

Logistic 函数定义为
$$
𝜎(𝑥) = \frac 1 {1 + exp(−x)}
$$


Logistic 函数可以看成是一个**“挤压”函数**，把一个实数域的输入“挤压”到(0, 1)．当**输入值**在**0** 附近时，Sigmoid 型函数近似为**线性函数**；当输入值**靠近两端**时，对输入进行**抑制**．输入越小，越接近于0；输入越大，越接近于1．且Logistic函数**连续**并**可导**

![](https://i.loli.net/2020/09/19/jlDV8iwqW1pZI7u.png)



因为Logistic 函数的性质，使得**装备了Logistic 激活函数的神经元**具有以下**两点性质**：

1. 其**输出**直接可以看作**概率分布**，使得神经网络可以更好地和统计学习模型进行结合．
2. 其可以看作一个**软性门**（Soft Gate），用来**控制其他神经元输出信息**的**数量**．



##### 4.1.1.2 Tanh函数



**Tanh 函数**也是一种Sigmoid 型函数．其定义为
$$
tanh(𝑥) = \frac {exp(𝑥) − exp(−𝑥)} {exp(𝑥) + exp(−𝑥)}
$$

Tanh 函数可以看作**放大**并**平移**的**Logistic 函数**，其值域是(−1, 1)．



Tanh 函数的输出是**零中心化**的（Zero-Centered），而Logistic函数的输出恒大于0．**非零中心化的输出**会使得其**后一层**的神经元的**输入**发生**偏置偏移**（Bias Shift），并进一步使得梯度下降的**收敛速度变慢**．

![](https://i.loli.net/2020/09/19/3a7nWT2rzLSydfp.png)

##### 4.1.1.3 Hard-Logistic函数和Hard-Tanh函数

Logistic 函数和Tanh 函数都是Sigmoid 型函数，具有饱和性，但是**计算开销较大**．因为这两个函数都是在中间（0 附近）**近似线性**，**两端饱和**．因此，这两个函数可以通过**分段函数来近似**．



![](https://i.loli.net/2020/09/19/Zf9VRq8Dmk75Cru.png)



#### 4.1.2 ReLU函数



ReLU（Rectified Linear Unit，**修正线性单元**）[Nair et al., 2010]，也叫Rectifier 函数[Glorot et al., 2011]，是目前深度神经网络中经常使用的激活函数．
ReLU 实际上是一个**斜坡（ramp）函数**，定义为

![](https://i.loli.net/2020/09/19/YVgEHFLeslTi9qK.png)



**优点**

- 采用ReLU 的神经元只需要进行加、乘和比较的操作，**计算**上更加**高效**．
- ReLU 函数也被认为具有**生物学合理性**（Biological Plausibility），比如单侧抑制、宽兴奋边界（即兴奋程度可以非常高）．
- 在优化方面，相比于Sigmoid 型函数的两端饱和，ReLU 函数为左饱和函数，且在𝑥 > 0 时导数为1，在一定程度上**缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度**．

**缺点**

- ReLU 函数的输出是**非零中心化**的，给后一层的神经网络引入偏置偏移，会**影响梯度下降的效率**．
- ReLU 神经元在训练时比较容易“死亡”．在训练时，如果参数在一次**不恰当的更新后**，第一个隐藏层中的**某个ReLU 神经元在所有的训练数据上都不能被激活**，那么这个神经元自身参数的**梯度永远都会是0**，在以后的训练过程中永远不能被激活．这种现象称为**死亡ReLU问题**（Dying ReLU Problem），并且也有可能会发生在其他隐藏层． 



在实际使用中，为了避免上述情况，有几种ReLU 的变种也会被广泛使用．



##### 4.1.2.1 带泄露的ReLU



带泄露的ReLU（Leaky ReLU）在**输入𝑥 < 0** 时，也**保持**一个很**小的梯度𝛾**．这样当神经元**非激活时**也能有一个非零的梯度**可以更新参数**，**避免死亡ReLU问题**

![](https://i.loli.net/2020/09/19/gjBWVaq9pryM2XL.png)

##### 4.1.2.2 带参数的ReLU



带参数的ReLU（Parametric ReLU，PReLU）**引入**一个**可学习的参数**，不同神经元可以有不同的参数[He et al., 2015]．对于第𝑖 个神经元，其PReLU 的定义为

![](https://i.loli.net/2020/09/19/jWlxOcsJy5zeugf.png)

如果𝛾𝑖 = 0，那么PReLU 就退化为ReLU．如果𝛾𝑖 为一个很小的常数，则PReLU 可以看作带泄露的ReLU．PReLU 可以**允许不同神经元**具有**不同的参数**，也可以一组神经元**共享一个参数**．



##### 4.1.2.3 ELU函数



**ELU**（Exponential Linear Unit，**指数线性单元**）是一个近似的**零中心化**的**非线性函数**，其定义为



![](https://i.loli.net/2020/09/19/xe2dMsOFZ7BjkTq.png)



其中**𝛾 ≥ 0** 是一个**超参数**，决定𝑥 ≤ 0 时的饱和曲线，并调整输出均值在0 附近．



##### 4.1.2.4 Softplus函数



**Softplus** 函数[Dugas et al., 2001] 可以看作**Rectifier 函数**的**平滑版本**，其定义为
$$
Softplus(𝑥) = log(1 + exp(x))
$$

Softplus 函数其导数刚好是Logistic 函数．Softplus 函数虽然也具有**单侧抑制**、**宽兴奋边界**的特性，却**没有稀疏激活性**．



下图给出了ReLU、Leaky ReLU、ELU 以及Softplus 函数的示例

![](https://i.loli.net/2020/09/19/L4aEW5zecYCRFMf.png)



#### 4.1.3 Swish函数



Swish 函数是一种**自门控（Self-Gated）**激活函数，定义为
$$
swish(𝑥) = x𝜎(𝛽x)
$$
其中**𝜎(⋅) 为Logistic 函数**，**𝛽** 为可学习的**参数**或一个固定**超参数**．𝜎(⋅) ∈ (0, 1) 可以看作一种软性的门控机制．当**𝜎(𝛽𝑥) 接近于1** 时，门处于**“开”状态**，激活函数的**输出近似于𝑥 本身**；当**𝜎(𝛽𝑥) 接近于0** 时，门的状态为**“关”**，激活函数的**输出近似于0**．



![](https://i.loli.net/2020/09/19/jw3n7ZMKSGIk421.png)



1. 当𝛽 = 0 时，Swish 函数变成线性函数𝑥/2．
2. 当𝛽 = 1 时，Swish 函数在𝑥 > 0时近似线性，在𝑥 < 0 时近似饱和，同时具有一定的非单调性．
3. 当𝛽 → +∞ 时，𝜎(𝛽𝑥) 趋向于离散的0-1 函数，Swish 函数近似为ReLU 函数．
4. 因此，Swish 函数可以看作**线性函数**和**ReLU 函数**之间的**非线性插值函数**，其**程度**由**参数𝛽 控制**．



#### 4.1.4 GELU函数

**GELU**也是一种通过**门控机制**来调整其输出值的激活函数，和Swish 函数比较类似．
$$
GELU(𝑥) = 𝑥𝑃(𝑋 ≤ 𝑥)
$$


- 其中𝑃(𝑋 ≤ 𝑥) 是**高斯分布**𝒩(𝜇, 𝜎2) 的**累积分布函数**，其中𝜇, 𝜎 为超参数，一般设𝜇 = 0, 𝜎 = 1 即可．
- 由于**高斯分布的累积分布函数为S 型函数**，因此GELU 函数可以**用Tanh 函数或Logistic 函数来近似**，

![](https://i.loli.net/2020/09/19/MkJdjsRatXLbANv.png)



- 当**使用Logistic 函数来近似**时，GELU 相当于一种**特殊的Swish** 函数(𝛽值为1.702)．



#### 4.1.5 Maxout函数



Maxout 单元也是一种**分段线性函数**．Maxout 单元的**输入**是**上一层神经元**的**全部原始输出**，是一个向量𝒙 = [𝑥1; 𝑥2; ⋯ ; 𝑥𝐷]．
**每个Maxout 单元**有**𝐾 个权重向量**$𝒘_𝑘 ∈ ℝ^𝐷$ 和**K个偏置**$𝑏_𝑘 (1 ≤ 𝑘 ≤ 𝐾)$．对于输入𝒙，可以得到**𝐾 个净输入**$𝑧_𝑘$, 1 ≤ 𝑘 ≤ 𝐾．


$$
z_k = w^T_kx + b_k
$$


其中$𝒘_𝑘 = [𝑤_{𝑘,1}, ⋯ , 𝑤_{𝑘,𝐷}]^T$ 为第𝑘 个权重向量．



然后从这个K个净输入中**挑选出最大值**作为**输出**
$$
maxout(x) = max_{k \in [1, K]}(z_k)
$$
Maxout 单元**不单是净输入到输出之间的非线性映射**，而是整体学习**输入到输出之间的非线性映射**关系．



### 4.2 网络结构



要想模拟人脑的能力，单一的神经元是远远不够的，需要通过**很多神经元一起协作**来**完成复杂的功能**．这样**通过一定的连接方**
**式**或信息传递方式进行**协作**的**神经元**可以看作一个**网络**，就是神经网络．



到目前为止，研究者已经发明了各种各样的神经网络结构．目前常用的神经网络结构有以下三种：



![](https://i.loli.net/2020/09/19/AHc3O1QUw96pT24.png)



#### 4.2.1 前馈网络



- 前馈网络中各个**神经元**按**接收信息的先后**分为不同的**分组**．
- 每一组可以看作一个神经层．**每一层**中的神经元**接收前一层**神经元的**输出**，并**输出**到**下一层**神经元．
- 整个网络中的**信息**是**朝一个方向传播**，没有反向的信息传播，可以用一个**有向无环路图**表示．
- 前馈网络包括**全连接前馈网络**（本章中的第4.3节）和**卷积神经网络**（第5章）等．
- 前馈网络可以看作一个**函数**，通过**简单非线性函数**的多次**复合**，实现**输入空间**到**输出空间**的**复杂映射**．这种网络结构简单，易于实现．



#### 4.2.2 记忆网络



- **记忆网络**，也称为**反馈网络**，网络中的神经元不但可以**接收其他神经元的信息**，也可以**接收自己的历史信息**．
- 和前馈网络相比，记忆网络中的神经元**具有记忆功能**，在不同的时刻具有不同的状态．
- 记忆神经网络中的**信息传播可以是单向或双向传递**，因此可用一个**有向循环图**或**无向图**来表示．
- 记忆网络包括**循环神经网络**（第6章）、Hopfield 网络（第8.6.1节）、玻尔兹曼机（第12.1节）、受限玻尔兹曼机（第12.2节）等．



#### 4.2.3 图网络



前馈网络和记忆网络的输入都可以表示为向量或向量序列．但实际应用中，很多数据是**图结构的数据**，比如知识图谱、社交网络、分子（Molecular ）网络等．前馈网络和记忆网络很难处理图结构的数据．

图网络是**定义在图结构数据上**的**神经网络**（第6.8节）．图中每个节点都由一个或一组神经元构成．**节点之间的连接**可以是**有向**的，也可以是**无向**的．每个节点可以**收到来自相邻节点或自身的信息**．



图网络是**前馈网络**和**记忆网络**的**泛化**，包含很多不同的实现方式，比如**图卷积网络**、**图注意力网络**、**消息传递神经网络**等．



### 4.3 前馈神经网络



不同的**神经网络模型**有着不同**网络连接的拓扑结构**．一种比较**直接的拓扑结构是前馈网络**．**前馈神经网络**（Feedforward Neural Network，FNN）是**最早发明的简单人工神经网络**．前馈神经网络也经常称为**多层感知器**（Multi-Layer Perceptron，MLP）．但多层感知器的叫法并不是十分合理，因为**前馈神经网络**其实是由**多层的Logistic 回归模型**（连续的非线性函数）组成，而**不是**由**多层的感知器**（不连续的非线性函数）组成．



在前馈神经网络中，**各神经元**分别属于**不同的层**．

- 每一层的**神经元**可以**接收前一层神经元**的信号，并产生**信号输出**到下一层．
- **第0层**称为**输入层**，**最后一层**称为**输出层**，**其他中间层**称为**隐藏层**．
- 整个**网络中无反馈**，信号从输入层向输出层单向传播，可用一个有向无环图表示．



下图给出前馈神经网络的示例

![](https://i.loli.net/2020/09/19/iwEetKZWBPx4vNa.png)



下表给出了描述前馈神经网络的数学符号

| 记号                   | 含义                                 |
| ---------------------- | ------------------------------------ |
| $L$                    | 神经网络的层数                       |
| $M_l$                  | 第$l$层神经元的个数                  |
| 𝑓𝑙 (⋅)                 | 第𝑙 层神经元的**激活函数**           |
| $W(𝑙) ∈ ℝ^{𝑀_𝑙×𝑀_𝑙−1}$ | 第𝑙 − 1 层到第𝑙 层的**权重矩阵**     |
| $b(𝑙) ∈ ℝ^{𝑀_𝑙}$       | 第𝑙 − 1 层到第𝑙 层的**偏置**         |
| $z(l) ∈ ℝ^{𝑀_l}$       | 第𝑙 层神经元的**净输入**（净活性值） |
| $𝒂(𝑙) ∈ ℝ^{𝑀_l}$       | 第𝑙 层神经元的**输出**（活性值）     |



令$𝒂(0) = x$，**前馈神经网络**通过不断**迭代下面公式**进行**信息传播**：
$$
z^{(𝑙)} = 𝑾^{(𝑙)}𝒂^{(𝑙−1)} + 𝒃^{(𝑙)}
$$

$$
𝒂^{(𝑙)} = 𝑓_𝑙(z^{(𝑙)})
$$

1. 首先根据第𝑙−1层神经元的活性值（Activation）𝒂(𝑙−1) 计算出第𝑙 层神经元的**净活性值**（Net Activation）𝒛(𝑙)
2. 然后**经过一个激活函数**得到第𝑙 层神经元的**活性值**．
3. 因此，我们也可以把**每个神经层**看作一个**仿射变换**（Affine Transformation）和一个**非线性变换**．



这样，**前馈神经网络**可以通过**逐层的信息传递**，得到网络**最后的输出𝒂(𝐿)**．整个网络可以看作一个**复合函数𝜙(𝒙; 𝑾, 𝒃)**，将**向量𝒙** 作为**第1 层**的**输入𝒂(0)**，将**第𝐿 层的输出𝒂(𝐿)** 作为**整个函数的输出**．


$$
𝒙 = 𝒂(0) → 𝒛(1) → 𝒂(1) → 𝒛(2) → ⋯ → 𝒂(𝐿−1) → 𝒛(𝐿) → 𝒂(𝐿) = 𝜙(𝒙; 𝑾, 𝒃),
$$
其中𝑾, 𝒃 表示网络中**所有层**的**连接权重**和**偏置**．



#### 4.3.1 通用近似定理

![](https://i.loli.net/2020/09/19/lxfs4PIoKN78MqQ.png)



翻译一下，通用近似定理是说，对于**具有线性输出层**和**至少一个使用“挤压”性质的激活函数**的**隐藏层**组成的前馈神经网络，只要其隐藏层**神经元的数量足够**，它可以**以任意的精度**来**近似**任何一个定义在实数空间$ℝ^𝐷$中的**有界闭集函数**



#### 4.3.2 应用到机器学习



根据**通用近似定理**，神经网络在某种程度上可以作为一个**“万能”函数**来使用，可以用来**进行复杂的特征转换**，或逼近一个复杂的条件分布．



要**取得好的分类效果**，需要将样本的**原始特征向量**𝒙 **转换**到**更有效的特征向量𝜙(𝒙)**，这个过程叫作**特征抽取**．



##### 4.3.2.1 观点一



多层前馈神经网络可以看作一个**非线性复合函数**$𝜙 ∶ ℝ^𝐷 → ℝ^{𝐷′}$，将输入$𝒙 ∈ ℝ^𝐷$ **映射**到输出$𝜙(𝒙) ∈ ℝ^𝐷′$．因此，多层前馈神经网络也可以看成是一种**特征转换**方法，其**输出𝜙(𝒙)** 作为**分类器的输入**进行分类．



给定一个训练样本(𝒙, 𝑦)，**先利用**多层前馈**神经网络**将𝒙 **映射**到𝜙(𝒙)，然后**再**将𝜙(𝒙) 输入到**分类**器𝑔(⋅)，即
$$
\hat y = g(\phi(x); \theta)
$$

- 其中𝑔(⋅)为线性或非线性的**分类器**
- 𝜃为分类器𝑔(⋅)的**参数**，
- $\hat y$为分类器的**输出**．



##### 4.3.2.2 观点二



如果分类器𝑔(⋅) 为Logistic 回归分类器或Softmax 回归分类器，那么**𝑔(⋅)** 也可以看成是**网络的最后一层**，即神经网络**直接输出**不同类别的**条件概率𝑝(𝑦|𝒙)**．



#### 4.3.3 参数学习



对于**某个样本**(𝒙, 𝑦)，如果采用**交叉熵损失函数**，其损失函数为
$$
L(y, \hat y) = -y^T log \hat y
$$
其中𝒚 ∈ {0, 1}𝐶 为标签𝑦 对应的one-hot 向量表示．



那么，对于给定训练集，为$D = {(𝒙(𝑛), 𝑦(𝑛))}^𝑁_{𝑛=1}$，将每个样本𝒙(𝑛) 输入给前馈神经网络，得到**网络输出**为$\hat y(n)$，其在**数据集$D$**上的**结构化风险函数**为
$$
R(𝑾, 𝒃) = \frac 1 N \sum ^N _{n=1} L(y^{(n)}, \hat y ^{(n)}) + \frac 1 2 \lambda ||W||^2_F
$$

- 其中𝑾 和𝒃 分别表示网络中所有的**权重矩阵**和**偏置向量**；
- $‖𝑾‖^2_F$ 是**正则化项**，用来**防止过拟合**；
- **𝜆** > 0 为**超参数**．𝜆 越大，𝑾 越接近于0



这里的$‖𝑾‖^2_F$一般使用 **Frobenius 范数**：
$$
||W||^2_F = \sum ^L _{l=1} \sum ^{M_l} _{i=1} \sum ^{M_{l-1}} _{j=1}(w_{ij}^{(l)})^2 
$$


有了学习准则和训练样本，网络参数可以通过**梯度下降法**来进行学习．在梯度下降方法的每次迭代中，第𝑙 层的参数𝑾(𝑙) 和𝒃(𝑙) **参数更新方式**为

![](https://i.loli.net/2020/09/20/aSCTGtPN4EZuQgn.png)



由上述公式可得，**梯度下降法**需要**计算损失函数对参数的偏导数**，如果通过链式法则逐一对每个参数进行求偏导比较低效．在神经网络的训练中经常使用**反向传播算法**来**高效地计算梯度**．



### 4.4 反向传播算法



使用误差**反向传播算法**的前馈神经网络训练过程可以分为以下**三步**：
（1） **前馈计算**每一层的净输入𝒛(𝑙) 和激活值𝒂(𝑙)，直到最后一层；
（2） **反向传播计算**每一层的**误差项𝛿(𝑙)**；
（3） 计算**每一层参数**的**偏导数**，**并更新**参数．



![](https://i.loli.net/2020/09/20/GFn5vWrJKdBqH83.png)



偏导数$\frac {𝜕L(𝒚, 𝒚̂)}{𝜕𝒛(𝑙)}$ 表示**第𝑙 层神经元对最终损失**的**影响**，也反映了最终损失对第𝑙 层神经元的**敏感程度**，因此一般称为第𝑙 层神经
元的**误差项**，用**𝛿(𝑙)** 来表示．
$$
𝛿(𝑙) ≜ \frac {𝜕L(𝒚, 𝒚̂)}{𝜕𝒛(𝑙)} ∈ ℝ^{𝑀_𝑙}
$$


误差项**𝛿(𝑙)** 也间接反映了**不同神经元对网络能力的贡献程度**，从而比较好地**解决**了**贡献度分配问题**（Credit Assignment Problem，CAP）．

### 4.5 自动梯度计算



参数的**梯度**可以让计算机来**自动计算**．目前，主流的深度学习框架都包含了自动梯度计算的功能，即我们可以**只考**
**虑网络结构**并用代码实现，其**梯度**可以自动进行计算，**无须人工干预**，这样可以大幅提高开发效率．



**自动计算梯度的方法**可以分为以下三类：**数值微分**、**符号微分**和**自动微分**．



#### 4.5.1 数值微分



**数值微分**（Numerical Differentiation）是用**数值方法**来**计算函数𝑓(𝑥) 的导数**．函数𝑓(𝑥) 的点𝑥 的导数定义为
$$
𝑓′(𝑥) = lim _{Δ𝑥→0} \frac {𝑓(𝑥 + Δ𝑥) − 𝑓(𝑥)} {Δ𝑥}
$$

要计算函数𝑓(𝑥) 在点𝑥 的导数，可以**对𝑥 加上**一个**很少的非零的扰动Δ𝑥**，通过上述定义来直接计算函数𝑓(𝑥) 的梯度．**数值微分**方法非常**容易实现**，但**找到一个合适的扰动**Δ𝑥 却十分**困难**．如果**Δ𝑥 过小**，会引起数值计算问题，比如**舍入误差**；如果**Δ𝑥 过大**，会增加**截断误差**，使得导数计算不准确．因此，数值微分的**实用性比较差**．



#### 4.5.2 符号微分



**符号微分**（Symbolic Differentiation）是一种**基于符号计算**的**自动求导**方法。



**符号计算**也叫代数计算，是指用计算机来**处理带有变量的数学表达式**．这里的**变量**被看作**符号**（Symbols），一般不需要代入具体的值．和符号计算相对应的概念是**数值计算**，即将数值代入数学表示中进行计算．



符号计算的**输入**和**输出**都是**数学表达式**，一般包括对数学表达式的**化简**、**因式分解**、**微分**、**积分**、解代数方程、求解常微分方程**等运算**．



例如数学公式的**化简**
$$
输入: 3x - x + 2x + 1
$$

$$
输出: 4x + 1
$$



- 符号微分可以在**编译时**就**计算梯度**的**数学表示**，并进一步利用符号计算方法进行优化．
- 此外，符号计算的一个优点是符号计算和**平台无关**，可以在CPU 或GPU 上运行．
- 符号微分也有一些不足之处：
  1. **编译时间较长**，特别是对于循环，需要很长时间进行编译；
  2. 为了进行符号微分，一般**需要设计一种专门的语言**来表示数学表达式，并且要对变量（符号）进行预先声明；
  3. **很难**对程序进行**调试**





#### 4.5.3 自动微分



**自动微分**（Automatic Differentiation，AD）是一种可以**对一个（程序）函数**进行**计算导数**的方法．符号微分的处理对象是数学表达式，而自动微分的**处理对象**是**一个函数**或**一段程序**．



自动微分的基本原理是**所有的数值计算**可以**分解**为一些基本操作，包含+, −, ×, / 和一些初等函数exp, log, sin, cos 等，然后利用**链式法则**来**自动计算**一个复合函数的**梯度**．



为简单起见，这里以一个神经网络中常见的复合函数的例子来说明自动微分的过程．令复合函数𝑓(𝑥; 𝑤, 𝑏) 为
$$
𝑓(𝑥; 𝑤, 𝑏) = \frac 1 {exp ( − (𝑤𝑥 + 𝑏)) + 1}
$$
其中**𝑥** 为输入**标量**，𝑤 和𝑏 分别为**权重**和**偏置参数**．



首先，我们将**复合函数**𝑓(𝑥; 𝑤, 𝑏) **分解**为一系列的基本操作，并**构成一个计算图**（Computational Graph）．**计算图**是**数学运算**的**图形化表示**．计算图中的**每个非叶子节点**表示一个**基本操作**，每个**叶子节点**为一个**输入变量**或**常量**

![](https://i.loli.net/2020/09/20/qni3wbDWVdSCF6T.png)



从计算图上可以看出，**复合函数**𝑓(𝑥; 𝑤, 𝑏) 由6 个**基本函数**ℎ𝑖, 1 ≤ 𝑖 ≤ 6 组成．如表4.2所示，**每个基本函数的导数都十分简单**，可以**通过规则**来**实现**．

![](https://i.loli.net/2020/09/20/Zklqch9vOgiLeW7.png)



整个复合函数𝑓(𝑥; 𝑤, 𝑏) 关于**参数𝑤 和𝑏 的导数**可以通过**计算图上的节点𝑓(𝑥; 𝑤, 𝑏)** 与参数𝑤 和𝑏 之间**路径上**所有的**导数连乘**来得到，即

![](https://i.loli.net/2020/09/20/zFOikKjRu5AMHtX.png)



##### 4.5.3.1 前向模式和反向模式



按照计算导数的顺序，自动微分可以分为两种模式：前向模式和反向模式．



**前向模式**：前向模式是按计算图中计算方向的相同方向来递归地计算梯度．

![](https://i.loli.net/2020/09/20/6WBq4GyKbZJXuNl.png)

**反向模式**：反向模式是按计算图中计算方向的相反方向来递归地计算梯度．

![](https://i.loli.net/2020/09/20/EP2eiAv3VLj7bDm.png)



对于一般的函数形式𝑓 ∶ $ℝ^N → ℝ^M$，**前向模式**需要对**每一个输入变量**都进行一遍**遍历**，共需要**𝑁 遍**．而**反向模式**需要对**每一个输出**都进行一个**遍历**，共需要**𝑀 遍**．当𝑁 > 𝑀 时，反向模式更高效．在**前馈神经网络**的**参数学习中**，风险函数为𝑓 ∶ $ℝ^N → ℝ$，**输出为标量**，因此采用**反向模式**为最有效的计算方式，**只需要一遍计算**．



##### 4.5.3.2 静态计算图和动态计算图



计算图按构建方式可以分为**静态计算图**（Static Computational Graph）和**动态计算图**（Dynamic Computational Graph）



**静态计算图**是在**编译时构建**计算图，计算图构建好之后在程序运行时不能改变，而**动态计算图**是在**程序运行时**动态构建．



两种构建方式各有优缺点．

- **静态计算图**在**构建时**可以进行**优化**，**并行能力强**，但**灵活性比较差**．
- **动态计算图**则**不容易优化**，当不同输入的网络结构不一致时，**难以并行**计算，但是**灵活性比较高**．



在目前深度学习框架里，Theano 和Tensorflow采用的是静态计算图，而DyNet、Chainer 和PyTorch 采用的是动态计算图．Tensorflow 2.0 也支持了动态计算图．



### 4.6 优化问题

神经网络的**参数学习**比线性模型要更加**困难**，主要原因有两点：

1. **非凸优化**问题
2. **梯度消失**问题．



#### 4.6.1 非凸优化问题



先介绍**凸**和**非凸**区别

**凸：**

- 指的是顺着梯度方向走到底就 **一定是 最优解** 。
- **大部分 传统机器学习 问题** 都是凸的。

![](https://i.loli.net/2020/09/20/3R6JcGWoM4bzudm.jpg)

**非凸：**

- 指的是顺着梯度方向走到底**只能保证是局部最优**，**不能保证** 是**全局最优**。
- **深度学习**以及小部分传统机器学习问题都是非凸的。

![](https://i.loli.net/2020/09/20/ZswqEJbfoyMANpX.jpg)



**神经网络**的优化问题是一个**非凸优化问题**



#### 4.6.2 梯度消失问题



在神经网络中误差反向传播的迭代公式为
$$
𝛿(𝑙) = 𝑓′_𝑙 (z^{(𝑙)}) ⊙ (𝑾^{(𝑙+1)})^T𝛿(𝑙+1).
$$
误差从输出层**反向传播**时，在**每一层**都要**乘以**该层的**激活函数**的**导数**．



但**是Sigmoid 型函数**的**导数**的**值域**都**小**于或等于1，由于Sigmoid 型函数的饱和性，饱和区的导数更是接近于0．这样，**误差**经过每一层传递都会不断**衰减**．当网络层数很深时，梯度就会不停衰减，甚至**消失**，使得整个网络很难训练．这就是所谓的**梯度消失问题**（Vanishing Gradient Problem），也称为**梯度弥散问题．**



**减轻梯度消失问题的方法**有很多种．一种简单有效的方式是**使用导数比较大的激活函数**，比如ReLU 等．