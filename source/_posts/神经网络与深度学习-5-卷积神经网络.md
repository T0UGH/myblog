---
title: '[神经网络与深度学习][5][卷积神经网络]'
date: 2020-11-11 17:14:32
categories:
    - 深度学习
tags:
    - 机器学习
---
## 第5章 卷积神经网络



> 一切都应该尽可能地简单，但不能过于简单



**卷积神经网络**（Convolutional Neural Network，CNN 或ConvNet）是一种具有**局部连接**、**权重共享**等特性的**深层前馈神经网络**．



**卷积神经网络**最早主要是用来**处理图像**信息．在用**全连接**前馈网络来处理图像时，会存在以下两个**问题**：

1. **参数太多**：会导致整个神经网络的**训练效率非常低**，也很容易出现**过拟合**．
2. 无法提取**局部不变性**特征：**自然图像**中的物体都具有**局部不变性特征**，比如尺度缩放、平移、旋转等操作不影响其语义信息．而**全连接前馈网络**很**难提取**这些局部不变性特征，一般需要进行数据增强来提高性能．



**卷积神经网络**是受生物学上**感受野机制**的启发而提出的．**感受野（Receptive Field）机制**主要是指**听觉、视觉**等神经系统中一些**神经元**的**特性**，即**神经元只接受其所支配的刺激区域内的信号**．例如，在视觉神经系统中，视觉皮层中的**神经细胞的输出**依赖于视网膜上的**光感受器**．视网膜上的**光感受器受刺激**兴奋时，将神经冲动**信号传到视觉皮层**，但**不是所有**视觉皮层中的神经元**都**会**接受**这些信号．一**个神经元的感受野是指视网膜上的特定区域，只有这个区域内的刺激才能够激活该神经元．**



目前的卷积神经网络一般是由**卷积层**、**汇聚层**和**全连接层**交叉堆叠而成的**前馈神经网络**．卷积神经网络有三个结构上的特性：**局部连接**、**权重共享**以及**汇聚**．这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性．和前馈神经网络相比，卷积神经网络的参**数更少**．



卷积神经网络主要使用在**图像**和视频分析的各种任务（比如**图像分类**、**人脸识别**、**物体识别**、**图像分割**等）上，其准确率一般也远远超出了其他的神经网络模型．近年来卷积神经网络也广泛地应用到自然语言处理、推荐系统等领域．



### 5.1 卷积



#### 5.1.1 卷积的定义



**卷积**（Convolution），也叫褶积，是分析数学中一种重要的运算．在**信号处理**或**图像处理**中，经常使用一维或二维卷积．



##### 5.1.1.1 一维卷积



略



##### 5.1.1.2 二维卷积



给定一个**图像** $𝑿 ∈ ℝ^{𝑀×𝑁}$ 和一个**滤波器**$W ∈ ℝ^{𝑈×𝑉}$ ，一般$𝑈 << 𝑀, 𝑉 << 𝑁$，其**卷积**为
$$
y_{ij} = \sum ^U _{u=1} \sum ^V _{v=1} w_{uv}x_{i-u+1,j-u+1}
$$


输入信息𝑿 和滤波器𝑾 的**二维卷积**定义为
$$
𝒀 = 𝑾 ∗ 𝑿
$$




![](https://i.loli.net/2020/09/20/Tz9abiSFQjuUtOM.png)



**注意：卷积操作中，滤波器翻转了180度**，仔细看上图1和-1分别乘了哪个元素



在图像处理中，卷积经常作为特征提取的有效方法．一幅图像在经过卷积操作后得到结果称为**特征映射**（Feature Map）．图5.3给出在图像处理中几种常用的滤波器，以及其对应的特征映射．图中最上面的滤波器是常用的**高斯滤波器**，可以用来对图像进行**平滑去噪**；**中间**和最**下面**的滤波器可以用来**提取边缘特征**．



![](https://i.loli.net/2020/09/20/6x1W32BvaUmD7Fj.png)



#### 5.1.2 互相关



在计算卷积的过程中，需要进行卷积核翻转．在具体实现上，一般会以互相关操作来代替卷积，从而会减少一些不必要的操作或开销．



给定一个图像$𝑿 ∈ ℝ^{𝑀×𝑁}$ 和卷积核$𝑾 ∈ ℝ^{U×V}$ ，它们的**互相关**为
$$
𝑦_{𝑖𝑗} = \sum ^U _{u=1} \sum ^V _{v=1} w_{uv}x_{i+u-1,j+v-1}
$$


和公式(5.7) 对比可知，互相关和卷积的**区别**仅仅在于**卷积核是否进行翻转**．因此**互相关**也可以称为不翻转卷积．


$$
𝒀 = 𝑾 ⊗ 𝑿 = rot180(𝑾) ∗ 𝑿
$$


其中**⊗** 表示**互相关运算**，**rot180(⋅)** 表示**旋转180 度**，$Y ∈ ℝ^{M−U+1,N−V+1}$ 为**输出矩阵**



在**神经网络中使用卷积**是为了进行**特征抽取**，卷积核**是否进行翻转**和其**特征抽取的能力无关**．特别是当**卷积核**是**可学习**的参数时，**卷积**和**互相关**在**能力上**是**等价**的．因此，为了实现上（或描述上）的**方便**起见，我们**用互相关来代替卷积**．事实上，很多深度学习工具中卷积操作其实都是互相关操作．



#### 5.1.3 卷积的变种



在卷积的标准定义基础上，还可以引入卷积核的滑动**步长**和**零填充**来增加卷积的多样性，可以更灵活地进行特征抽取．



**步长**（Stride）是指**卷积核**在**滑动**时的时间**间隔**．图5.4a给出了步长为2 的卷积示例．

![](https://i.loli.net/2020/09/20/oEwzUepcQBLPtFk.png)



**零填充**（Zero Padding）是在**输入向量两端进行补零**．图5.4b给出了输入的两端各补一个零后的卷积示例

![](https://i.loli.net/2020/09/20/Me3hv6NRaoWG1H2.png)

  	

### 5.2 卷积神经网络



卷积神经网络一般由**卷积层**、**汇聚层**和**全连接层**构成．



#### 5.2.1 用卷积代替全连接



在**全连接**前馈神经网络中，权重矩阵的**参数**非常**多**，训练的**效率**会非常**低**．



如果采用**卷积**来**代替全连接**，第𝑙 层的**净输入$𝒛^{(𝑙)}$** 为第𝑙 − 1 层**活性值**$𝒂^{(𝑙−1)}$和**卷积核**$w^{(𝑙)} ∈ ℝ^K$的卷积，即


$$
z(𝑙) = w(𝑙) ⊗ a(𝑙−1) + b(𝑙),
$$


其中**卷积核**$w^{(𝑙)} ∈ ℝ^K$ 为**可学习**的**权重向量**，$𝑏^{(𝑙)} ∈ ℝ$ 为**可学习**的**偏置**．



卷积层有两个很重要的性质：

- **局部连接**：在卷积层（假设是第𝑙 层）中的**每一个神经元**都只和**下一层**（第𝑙 − 1层）中某个**局部窗口**内的神经元**相连**，构成一个**局部连接网络**．如图5.5b所示，卷积层和下一层之间的**连接数大大减少**，由原来的$𝑀^𝑙 × 𝑀^{𝑙−1}$个连接变为$𝑀^𝑙 × 𝐾$个连接，**𝐾 为卷积核大小．**
- **权重共享**：作为参数的卷积核𝒘(𝑙) 对于第𝑙 层的所有的神经元都是相同的，可以理解为一个卷积核只捕捉输入数据中的一种特定的局部特征



由于局部连接和权重共享，卷积层的参数只有一个**𝐾 维的权重$𝒘^{(𝑙)}$** 和**1 维的偏置**$𝑏^{(𝑙)}$，共𝐾 + 1 个参数．**参数个数**和**神经元的数量无关**．此外，第𝑙 层的神经元个数不是任意选择的，而是满足$𝑀^𝑙 = 𝑀^{𝑙−1} − 𝐾 + 1$．

![](https://i.loli.net/2020/09/21/KDRw6kNFHoZ5xTt.png)

#### 5.2.2 卷积层



**卷积层**的作用是**提取**一个**局部区域的特征**，**不同的卷积核**相当于**不同的特征提取器**．



为了更充分地**利用图像**的局部信息，通常将**神经元**组织为**三维结构的神经层**，其大小为**高度𝑀× 宽度𝑁× 深度𝐷**，由**𝐷 个𝑀 × 𝑁** 大小的**特征映射**构成．



> **特征映射**（Feature Map）为一幅图像（或其他特征映射）在**经过卷积提取到**的**特征**，**每个特征映射可以作为一类抽取的图像特征**．为了提高卷积网络的表示能力，可以在**每一层**使用**多个不同的特征映射**，以更好地表示图像的特征．



不失一般性，假设一个卷积层的结构如下：

1. **输入特征映射组**：$X ∈ ℝ^{𝑀×𝑁×𝐷}$ 为**三维张量**（Tensor），其中每个**切片（Slice）矩阵**$𝑿^𝑑 ∈ ℝ^{𝑀×𝑁}$为一个**输入特征映射**，1 ≤ 𝑑 ≤ 𝐷；
2. **输出特征映射组**：$Y ∈ ℝ^{𝑀′×𝑁′×𝑃}$ 为**三维张量**，其中每个**切片矩阵**$Y^𝑝 ∈ ℝ^{𝑀′×𝑁′}$ 为一个**输出特征映射**，1 ≤ 𝑝 ≤ 𝑃；
3. **卷积核**：$W ∈ ℝ^{𝑈×𝑉 ×𝑃×𝐷}$ 为**四维张量**，其中每个**切片矩阵**$𝑾^{p,d} ∈ ℝ^{𝑈×𝑉}$ 为一个**二维卷积核**，1 ≤ 𝑝 ≤ 𝑃, 1 ≤ 𝑑 ≤ 𝐷．



![](https://i.loli.net/2020/09/21/iy9bqvYMcXl6zZs.png)



为了计算**输出特征映射**$𝒀^p$，用**卷积核**$𝑾^{p,1}, 𝑾^{𝑝,2}, ⋯ , 𝑾^{𝑝,𝐷}$**分别**对**输入特征映射**$X^1, X^2, ⋯ , X^D$ 进行**卷积**，然后将卷积结果相加，并加上一个标量偏置𝑏得到卷积层的**净输入$Z^p$**，再经过**非线性激活函数**后得到**输出特征映射**𝒀𝑝．
$$
Z^p = W^p ⊗ X + 𝑏^𝑝 = \sum ^D _{d=1} W^{p,d} ⊗ X^d + 𝑏^𝑝
$$

$$
Y^p = f(Z^p)
$$

其中$W^p ∈ ℝ^{𝑈×𝑉 ×𝐷}$ 为**三维卷积核**，𝑓(⋅) 为**非线性激活函数**，一般用**ReLU 函数**．



整个计算过程如图5.7所示．如果希望卷积层输出P 个特征映射，可以将上述计算过程重复𝑃 次，得到𝑃 个输出特征映射$Y^1, Y^2, ⋯ , Y^𝑃$．

![](https://i.loli.net/2020/09/21/kWgUNAqT7SRJxbe.png)



在输入为$𝒳 ∈ ℝ^{𝑀×𝑁×𝐷}$，输出为$𝒴 ∈ ℝ^{𝑀′×𝑁′×𝑃}$的卷积层中，每一个**输出特征映射**都需要**𝐷 个卷积核**以及**一个偏置**．假设每个**卷积核的大小**为**𝑈 × 𝑉**，那么共需要$𝑃 × 𝐷 × (𝑈 × 𝑉) + 𝑃$ 个参数．



#### 5.2.3 汇聚层



**汇聚层**（Pooling Layer）也叫**子采样层**（Subsampling Layer），其作用是进行特征选择，降低特征数量，从而减少参数数量．



可以在卷积层之后加上一个汇聚层，从而降低特征维数，避免过拟合．



假设汇聚层的**输入特征映射组**为$X ∈ ℝ^{𝑀×𝑁×𝐷}$，对于其中**每一个特征映射**$𝑿^𝑑 ∈ ℝ^{𝑀×𝑁}$ , 1 ≤ 𝑑 ≤ 𝐷，将其**划分为很多区域**$𝑅^d_{m,n}$, 1 ≤ 𝑚 ≤ 𝑀′, 1 ≤ 𝑛 ≤ 𝑁′，这些区域可以重叠，也可以不重叠．汇聚（Pooling）是指对**每个区域**进行**下采样**（Down Sampling）得到一个值，作为这个区域的概括．



常用的汇聚函数有两种：**最大汇聚**和**平均汇聚**



![](https://i.loli.net/2020/09/21/NqjWRuzdJO7I4Mn.png)



图5.8给出了采样最大汇聚进行子采样操作的示例．可以看出，**汇聚层**不但可以**有效地减少神经元的数量**，还可以使得网络对一些小的局部形态改变**保持不变性**，并拥有**更大的感受野**．



典型的汇聚层是将每个特征映射划分为**2 × 2 大小**的**不重叠区域**，然后使用**最大汇聚**的方式进行下采样．**过大**的采样区域会**急剧减少**神经元的**数量**，也会造成**过多**的**信息损失**．



#### 5.2.4 卷积网络的整体结构



一个典型的卷积网络是由**卷积层**、**汇聚层**、**全连接层**交叉堆叠而成．目前常用的卷积网络整体结构如图5.9所示．一个**卷积块**为连续𝑀 个卷积层和𝑏 个汇聚层（𝑀 通常设置为2 ∼ 5，𝑏 为0 或1）．一个卷积网络中可以堆叠𝑁 个连续的卷积块，然后在后面接着𝐾 个全连接层（𝑁 的取值区间比较大，比如1 ∼ 100 或者更大；𝐾 一般为0 ∼ 2）．



![](https://i.loli.net/2020/09/21/cu7VIMOBEYp2kli.png)



目前，卷积网络的**整体结构**趋向于使用**更小的卷积核**（比如1 × 1 和3 × 3）以及**更深的结构**（比如层数大于50）．此外，由于卷积的操作性越来越灵活（比如不同的步长），汇聚层的作用也变得越来越小，因此目前比较流行的卷积网络中，**汇聚层的比例**正在逐渐**降低**，趋向于**全卷积网络**．



### 5.3 参数学习



在**卷积网络**中，**参数**为**卷积核中权重**以及**偏置**．和全连接前馈网络类似，**卷积网络**也可以通过**误差反向传播算法**来进行参数**学习**．



不失一般性，对第𝑙 层为卷积层，**第𝑙 − 1 层**的**输入特征映射**为$X^{(𝑙−1)} ∈ℝ^{𝑀×𝑁×𝐷}$，通过卷积计算得到**第𝑙 层的特征映射净输入**$Z^{(𝑙)} ∈ ℝ^{𝑀′×𝑁′×𝑃}$．**第𝑙层**的**第𝑝(1 ≤ 𝑝 ≤ 𝑃) 个**特征映射**净输入**
$$
Z^{(l,p)} = \sum ^D _{d=1} W^{(l,p,d)} ⊗ X^{(l-1, d)} + b^{(l,p)}
$$
在卷积网络中，每层参数的**梯度** **依赖于** 其所在层的**误差项**$𝛿^{(𝑙,𝑝)}$.$𝛿^{(𝑙,𝑝)} = \frac {𝜕L} {𝜕𝒁^{(𝑙,𝑝)}}$ 为**损失函数**关于**第𝑙 层**的**第𝑝 个特征映射**净输入𝒁(𝑙,𝑝) 的**偏导数**．



损失函数$L$关于**第𝑙层**的**卷积核**$𝑾^{(𝑙,𝑝,𝑑)}$的**偏导数**为
$$
\frac {𝜕L} {𝜕𝑾(𝑙,𝑝,𝑑)} = \frac {𝜕ℒ} {𝜕𝒁(𝑙,𝑝)} ⊗ 𝑿^{(𝑙−1,𝑑)} = 𝛿^{(𝑙,𝑝)} ⊗ 𝑿^{(𝑙−1,𝑑)}
$$




损失函数关于**第𝑙 层**的**第𝑝 个偏置𝑏(𝑙,𝑝)** 的**偏导数**为
$$
\frac {𝜕L} {𝜕𝑏(𝑙,𝑝)} = \sum _{i,j} [𝛿^{(𝑙,𝑝)}]_{𝑖,𝑗}
$$




#### 5.3.1 卷积神经网络的反向传播算法



卷积层和汇聚层中误差项的计算有所不同，因此我们分别计算其误差项．



##### 5.3.1.1 汇聚层误差项计算



第𝑙 层的第𝑝 个特征映射的误差项𝛿(𝑙,𝑝) 的具体推导过程如下：

![](https://i.loli.net/2020/09/22/THvSnIouM5K4s9O.png)



其中$𝑓′_𝑙 (⋅)$ 为第𝑙 层使用的激活函数导数，up 为上采样函数（up sampling），与汇聚层中使用的下采样操作刚好相反．



##### 5.3.1.2 卷积层误差项计算



第𝑙 层的第𝑑 个特征映射的误差项𝛿(𝑙,𝑑) 的具体推导过程如下

![](https://i.loli.net/2020/09/22/eaP91QKCDZXjcVo.png)

其中$\hat ⊗ $为**宽卷积**．



### 5.4 几种典型的卷积神经网络



#### 5.4.1 LeNet-5



基于LeNet-5 的手写数字识别系统在20 世纪90 年代被美国很多银行使用，用来识别支票上面的手写数字．**LeNet-5** 的**网络结构**如图5.10所示

![](https://i.loli.net/2020/09/22/hFTgKBrd3a5LOcp.png)



LeNet-5 共有**7 层**，接受**输入图像大小**为**32 × 32** = 1 024，**输出**对应**10 个类别的得分**．LeNet-5 中的每一层结构如下：

1. **C1层**是**卷积层**，使用**6个5×5的卷积核**，得到6组大小为28×28 = 784的特征映射．因此，C1 层的神经元数量为6 × 784 = 4 704，可训练参数数量为6 × 25 + 6 = 156，连接数为156 × 784 = 122 304．
2. **S2** 层为**汇聚层**，**采样窗口为2 × 2**，使用**平均汇聚**，并使用一个非线性函数．神经元个数为6 × 14 × 14 = 1 176，可训练参数数量为
   6 × (1 + 1) = 12，连接数为6 × 196 × (4 + 1) = 5 880．
3. **C3** 层为**卷积层**．LeNet-5 中用一个**连接表**来定义**输入**和**输出特征映射之间**的**依赖关系**，如图5.11所示，共使用60 个5 × 5 的卷积核，得到16 组大小为10 × 10 的特征映射．（如果不使用连接表，则需要6 × 16 = 96 个5 × 5 的卷积核．）神经元数量为16 × 100 = 1 600，可训练参数数量为(60 × 25) + 16 = 1 516，连接数为100 × 1 516 = 151 600．
4. **S4** 层是一个**汇聚层**，**采样窗口为2 × 2**，得到16 个5 × 5 大小的特征映射，可训练参数数量为16 × 2 = 32，连接数为16 × 25 × (4 + 1) = 2 000．
5. **C5 层**是一个**卷积层**，使用120 × 16 = 1 920 个**5 × 5 的卷积核**，得到120 组大小为1 × 1 的特征映射．C5 层的神经元数量为120，可训练参数数量为1 920 × 25 + 120 = 48 120，连接数为120 × (16 × 25 + 1) = 48 120．
6. **F6** 层是一个**全连接层**，有84 个神经元，可训练参数数量为84 × (120 +1) = 10 164．连接数和可训练参数个数相同，为10 164．
7. **输出层**：输出层由**10 个径向基函数**（Radial Basis Function，RBF）组成．这里不再详述．



##### 5.4.1.1 连接表



一般来说，卷积层的**每一个输出特征映射**都**依赖**于**所有输入特征映射**，相当于卷积层的**输入**和**输出**特征映射之间是**全连接**的关系．实际
上，这种**全连接关系不是必须的**．我们可以让**每一个输出特征映射**都**依赖**于**少数几个输入特征映射**．



定义一个连接表（Link Table）𝑇 来描述输入和输出特征映射之间的连接关系．在LeNet-5 中，连接表的基本设定如图5.11所示．

![](https://i.loli.net/2020/09/22/MPLN93ekRBHqcOT.png)

- C3 层的第0-5 个特征映射依赖于S2 层的特征映射组的**每3 个连续子集**，
- 第6-11 个特征映射依赖于S2 层的特征映射组的**每4 个连续子集**，
- 第12-14 个特征映射依赖于S2 层的特征映射的**每4 个不连续子集**，
- 第15 个特征映射依赖于S2 层的**所有**特征映射．



如果第𝑝 个输出特征映射依赖于第𝑑 个输入特征映射，则$T_{p,d} = 1$，否则为0．𝒀𝑝 为
$$
𝒀^p = 𝑓( \sum _{D, T_{p,d} = 1} W^{p,d} ⊗ 𝑿^d + b^p)
$$


其中𝑇 为𝑃 × 𝐷 大小的连接表．假设连接表𝑇 的非零个数为𝐾，每个卷积核的大小为𝑈 × 𝑉，那么共需要𝐾 × 𝑈 × 𝑉 + 𝑃 参数．



#### 5.4.2 AlexNet



**AlexNet**是第一个**现代深度卷积网络模型**，其首次使用了很多现代深度卷积网络的技术方法，比如**使用GPU** 进行**并行训练**，采用了**ReLU** 作为**非线性激活函数**，使用**Dropout** 防止**过拟合**，使用**数据增强**来**提高模型准确率**等．AlexNet 赢得了2012 年ImageNet 图像分类竞赛的冠军．



AlexNet 的结构如图5.12所示，包括**5 个卷积层**、**3 个汇聚层**和**3 个全连接层**（其中**最后一层**是使用**Softmax 函数**的**输出层**）．因为网络**规模超出**了当时的**单个GPU 的内存限制**，AlexNet 将网络**拆为两半**，分别**放在两个GPU 上**，GPU 间**只在某些层（比如第3 层）进行通信**．

![](https://i.loli.net/2020/09/22/LDTzsAWhecInyi7.png)

AlexNet 的**输入**为**224 × 224 × 3 的图像**，**输出**为**1 000 个类别的条件概率**，具体结构如下： 这里的卷积核使用四维张量来描述．

1. 第一个**卷积层**，使用两个大小为**11 × 11 × 3 × 48 的卷积核**，步长𝑆 = 4，零填充𝑃 = 3，得到两个大小为55 × 55 × 48 的特征映射组
2. 第一个**汇聚层**，使用大小为**3 × 3** 的**最大汇聚操作**，步长𝑆 = 2，得到两个27 × 27 × 48 的特征映射组．这里的汇聚操作是有重叠的，以提取更多的特征．
3. 第二个**卷积层**，使用两个大小为**5 × 5 × 48 × 128 的卷积核**，步长𝑆 = 1，零填充𝑃 = 2，得到两个大小为27 × 27 × 128 的特征映射组．
4. 第二个**汇聚层**，使用大小为**3 × 3 的最大汇聚操作**，步长𝑆 = 2，得到两个大小为13 × 13 × 128 的特征映射组．
5. 第三个**卷积层**为两个路径的融合，使用一个大小为3 × 3 × 256 × 384的卷积核，步长𝑆 = 1，零填充𝑃 = 1，得到两个大小为13 × 13 × 192 的特征映射组．
6. 第四个**卷积层**，使用两个大小为**3 × 3 × 192 × 192 的卷积核**，步长𝑆 = 1，零填充𝑃 = 1，得到两个大小为13 × 13 × 192 的特征映射组．
7. 第五个**卷积层**，使用两个大小为**3 × 3 × 192 × 128 的卷积核**，步长𝑆 = 1，零填充𝑃 = 1，得到两个大小为13 × 13 × 128 的特征映射组．
8. 第三个**汇聚层**，使用大小为**3 × 3 的最大汇聚操作**，步长𝑆 = 2，得到两
   个大小为6 × 6 × 128 的特征映射组．
9. **三个全连接层**，神经元数量分别为4 096、4 096 和1 000．此外，AlexNet 还在前两个汇聚层之后进行了**局部响应归一化**（Local Response Normalization，LRN）以**增强模型的泛化能力**．





#### 5.4.3 Inception网络



在Inception 网络中，一个卷积层包含多个不同大小的卷积操作，称为**Inception 模块**．Inception 网络是由有**多个Inception 模块**和**少量的汇聚层**堆叠而成．



Inception模块**同时使用**1 × 1、3 × 3、5 × 5等**不同大小的卷积核**，并将得到的特征映射**在深度上**拼接（**堆叠**）起来**作为输出特征映射**．



![](https://i.loli.net/2020/09/22/4ZiVTInY1CQj5Na.png)



Inception 网络有多个版本，其中最早的Inception v1 版本就是非常著名的GoogLeNetGoogLeNet 由9 个Inception v1 模块和5 个汇聚层以及其他一些卷积层和全连接层构成，总共为22 层网络，如图5.14所示．

![](https://i.loli.net/2020/09/22/LIn9KmtHbRvPODr.png)



**Inception v3** 网络用**多层的小卷积核**来**替换** **大的卷积核**，以**减少计算量**和**参数量**，并**保持感受野不变**．



#### 5.4.4 残差网络



**残差网络**（Residual Network，ResNet）通过**给非线性的卷积层增加直连边**（Shortcut Connection）（也称为**残差连接**（Residual Connection））的**方式**来**提高信息的传播效率**．



图5.15给出了一个典型的残差单元示例．残差单元由**多个级联的（等宽）卷积层**和一个**跨层的直连边**组成，**再经过ReLU 激活后**得到**输出**

![](https://i.loli.net/2020/09/22/CIqvVzGBXHRZPSy.png)



下面解释**残差结构**的**好处**

假设在一个深度网络中，我们期望一个**非线性单元**（可以为一层或多层的卷积层）𝑓(𝒙; 𝜃) 去**逼近**一个**目标函数为ℎ(𝒙)**．如果将目标函数拆分成两部分：**恒等函数**（Identity Function）𝒙 和**残差函数**（Residue Function）ℎ(𝒙) − 𝒙．
$$
h(x) = (x) + (h(x) - x)
$$


根据通用近似定理，一个**由神经网络构成的非线性单元**有**足够的能力**来近似**逼近原始目标函数**或**逼近残差函数**，但实际中**后者更容易学习**.因此，原来的优化问题可以转换为：让**非线性单元𝑓(𝒙; 𝜃)** 去**近似残差函数ℎ(𝒙) − 𝒙**，并**用𝑓(𝒙; 𝜃) + 𝒙** 去**逼近ℎ(𝒙)**．



### 5.5 其他卷积方式



#### 5.5.1 转置卷积



我们一般可以通过**卷积操作**来实现**高维特征**到**低维特征**的**转换**．比如在一维卷积中，一个5 维的输入特征，经过一个大小为3 的卷积核，其输出为3 维特征．但在一些任务中，我们需要将**低维特征**映射到**高维特征**，并且依然希望**通过卷积操作**来实现，这时我们可以采用**转置卷积**．



假设有一个**高维向量**为$𝒙 ∈ ℝ^d$ 和一个**低维向量**为 $z∈ ℝ^p$，𝑝 < 𝑑．如果用**仿射变换**（Affine Transformation）来实现**高维到低维的映射**


$$
z = Wx
$$


其中$W ∈ ℝ^{𝑝×𝑑}$ 为**转换矩阵**．



我们也可以很容易地通过**转置𝑾** 来实现**低维到高维**的反向映射
$$
x = W^Tz
$$


**卷积操作**也可以写为**仿射变换的形式**．假设一个5 维向量𝒙，经过大小为3的卷积核$𝒘 = [𝑤1, 𝑤2, 𝑤3]^T$ 进行卷积，得到3 维向量𝒛．卷积操作可以写为

![image-20200922132111858](C:\Users\MSI-NB\AppData\Roaming\Typora\typora-user-images\image-20200922132111858.png)



如果要实现3 维向量$z$到5 维向量$x$的映射，可以通过**仿射矩阵**的**转置**来**实现**，即

![](https://i.loli.net/2020/09/22/HLfaIpCTXBx7SwD.png)



从仿射变换的角度来看**两个卷积操作**𝒛 = 𝒘⊗𝒙和$x = rot180(w)\hat ⊗ z$也是**形式上的转置关系**． 因此，我们将低维特征映射到高维特征的卷积操作称为**转置卷积**（Transposed Convolution），也称为**反卷积**（Deconvolution）



对一个**𝑀 维**的向量𝒛，和**大小为𝐾 的卷积核**，如果希望通过卷积操作来映射到更高维的向量，只需要对**向量𝒛进行两端补零**𝑃 = 𝐾 − 1，然后进行**卷积**，可以得到**𝑀 + 𝐾 − 1 维的向量**．( 即**宽卷积**．)



##### 5.5.1.1 微步卷积



我们也可以通过**使转置卷积**的**步长𝑆 < 1**来实现**上采样**操作，大幅**提高特征维数**．步长𝑆 < 1 的转置卷积也称为**微步卷积**（Fractionally-Strided Convolution）．为了实现微步卷积，我们可以在**输入特征之间插入0** 来间接地使得步长变小．



#### 5.5.2 空洞卷积



对于一个卷积层，如果希望**增加输出单元**的**感受野**，一般可以通过三种方式实现：

1. **增加卷积核**的**大小**；

2. **增加层数**，比如两层3 × 3 的卷积可以近似一层5 × 5 卷积的效果；

3. 在卷积**之前**进行**汇聚操作**．

前两种方式会**增加参数数量**，而第三种方式会**丢失一些信息**．





**空洞卷积**（Atrous Convolution）是一种**不增加参数数量**， 同时**增加输出单元感受野**的一种方法，也称为**膨胀卷积**（Dilated Convolution）．



空洞卷积通过**给卷积核** **插入“空洞”**来变相地增加其大小．如果在卷积核的每两个元素之间插入𝐷 − 1 个空洞，卷积核的有效大小为
$$
𝐾′ = 𝐾 + (𝐾 − 1) × (𝐷 − 1),
$$


其中𝐷 称为**膨胀率**（Dilation Rate）．当**𝐷 = 1** 时卷积核为**普通的卷积核**．



图5.18给出了**空洞卷积**的示例．



![](https://i.loli.net/2020/09/22/XM2LqTftBI6418E.png)